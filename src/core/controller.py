import configparser
import json
import datetime
import re
import os
import glob
from pathlib import Path
from src.services.llm_service import (
    analyze_text, refine_sales_data, polish_text, 
    update_sales_data, classify_intent, query_sales_data, summarize_text
)
from src.services.asr_service import transcribe_audio
from src.services.vector_service import VectorService

class LinkSellController:
    def __init__(self, config_path="config/config.ini"):
        self.config = configparser.ConfigParser()
        self.config_path = Path(config_path)
        if self.config_path.exists():
            self.config.read(self.config_path)
        
        # 1. è®¾ç½®å…¨å±€ç¯å¢ƒå˜é‡ä¸é»˜è®¤è®°å½•è€…
        hf_endpoint = self.config.get("global", "hf_endpoint", fallback=None)
        if hf_endpoint:
            os.environ["HF_ENDPOINT"] = hf_endpoint
        
        self.default_recorder = self.config.get("global", "default_recorder", fallback="é™ˆä¸€éª")
            
        self.api_key = self.config.get("doubao", "api_key", fallback=None)
        self.endpoint_id = self.config.get("doubao", "analyze_endpoint", fallback=None)
        
        # ASR Config
        self.asr_app_id = self.config.get("asr", "app_id", fallback=None)
        self.asr_token = self.config.get("asr", "access_token", fallback=None)
        self.asr_resource = self.config.get("asr", "resource_id", fallback="volc.seedasr.auc")
        if self.asr_resource == "volc.bigasr.sauc.duration":
             self.asr_resource = "volc.seedasr.auc"

        # 2. åŠ è½½å•†æœºé˜¶æ®µæ˜ å°„
        self.stage_map = {}
        if self.config.has_section("opportunity_stages"):
            self.stage_map = {k: v for k, v in self.config.items("opportunity_stages")}

        # 3. åˆå§‹åŒ–æ•°æ®ç›®å½•
        self.data_dir = Path("data/opportunities")
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # 4. åˆå§‹åŒ–æœ¬åœ°å‘é‡åº“
        try:
            self.vector_service = VectorService()
        except Exception as e:
            print(f"[yellow]è­¦å‘Šï¼šæœ¬åœ°å‘é‡æ¨¡å‹åŠ è½½å¤±è´¥({{e}})é”›ï¿½å°†å›é€€åˆ°æ™®é€šæŸ¥è¯¢æ¨¡å¼éŠ†ï¿½[/yellow]")
            self.vector_service = None

    def validate_llm_config(self):
        return bool(self.api_key and self.endpoint_id and "YOUR_" not in self.api_key)

    def validate_asr_config(self):
        return bool(self.asr_app_id and self.asr_token and "YOUR_" not in self.asr_token)

    def transcribe(self, audio_file, debug=False):
        if not self.validate_asr_config():
            raise ValueError("ASR Configuration Invalid")
        return transcribe_audio(audio_file, self.asr_app_id, self.asr_token, self.asr_resource, debug=debug)

    def polish(self, text):
        if not self.validate_llm_config():
            raise ValueError("LLM Configuration Invalid")
        return polish_text(text, self.api_key, self.endpoint_id)

    def analyze(self, text):
        if not self.validate_llm_config():
            raise ValueError("LLM Configuration Invalid")
        return analyze_text(text, self.api_key, self.endpoint_id)

    def identify_intent(self, text):
        """è¯†åˆ«æ„å›¾ï¼šCREATE, LIST, GET, UPDATE, DELETE, OTHER"""
        if not self.validate_llm_config():
            return "CREATE"
        
        # è°ƒç”¨ LLM è¿›è¡Œåˆ†ç±»
        intent = classify_intent(text, self.api_key, self.endpoint_id)
        
        # ä¸¥æ ¼è§„èŒƒåŒ–
        valid_intents = ["CREATE", "LIST", "GET", "UPDATE", "DELETE", "OTHER"]
        if intent not in valid_intents:
            # å°è¯•å…³é”®è¯è¡¥æ•‘
            if any(k in text for k in ["æŸ¥", "æ‰¾", "çœ‹", "å“ªäº›", "æœç´¢"]): return "LIST"
            if any(k in text for k in ["åˆ ", "ç§»é™¤"]): return "DELETE"
            if any(k in text for k in ["æ”¹", "æ›´æ–°", "æ¢"]): return "UPDATE"
            return "CREATE"
            
        # OTHER çš„äººå·¥å¤æ ¸ï¼šé˜²æ­¢å¯¹ä¸šåŠ¡æŒ‡ä»¤çš„è¯¯æ€
        if intent == "OTHER":
            # ä¸šåŠ¡æ ¸å¿ƒè¯æ±‡åˆ—è¡¨
            biz_keywords = ["é¡¹ç›®", "å•†æœº", "å•å­", "å®¢æˆ·", "èŠ", "èŠè¿‡", "è°ˆ", "é¢„ç®—", "è¿›åº¦", "è·Ÿè¿›", "è¯¦æƒ…", "æ¡£æ¡ˆ"]
            if len(text) > 8 or any(k in text for k in biz_keywords):
                # å¦‚æœåŒ…å«ä¸šåŠ¡è¯ï¼Œé»˜è®¤è½¬ä¸ºæŸ¥è¯¢æ„å›¾ (LIST) æ¯”è¾ƒç¨³å¦¥ï¼Œè®©åç»­é€»è¾‘å»æœ
                return "LIST"
                
        return intent

    def extract_search_term(self, text):
        """
        ä½¿ç”¨ LLM ä»ç”¨æˆ·æŒ‡ä»¤ä¸­æå–æ ¸å¿ƒæœç´¢è¯ï¼ˆé¡¹ç›®åï¼‰ã€‚
        ä¾‹å¦‚ï¼š"æŸ¥çœ‹æ²ˆé˜³è½´æ‰¿å‚è¯¦æƒ…" -> "æ²ˆé˜³è½´æ‰¿å‚"
        """
        from volcenginesdkarkruntime import Ark
        
        client = Ark(api_key=self.api_key)
        # åŠ è½½ prompt
        prompt_path = Path("config/prompts/extract_search_term.txt")
        if not prompt_path.exists(): return text # Fallback
        
        with open(prompt_path, "r", encoding="utf-8") as f:
            sys_prompt = f.read()

        try:
            completion = client.chat.completions.create(
                model=self.endpoint_id,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": text},
                ],
                temperature=0.1, 
            )
            term = completion.choices[0].message.content.strip()
            if "Unknown" in term: return text
            # å»é™¤å¯èƒ½çš„å¼•å·å’Œåå¼•å·ï¼Œå¹¶ä¸”å‰åå»ç©ºæ ¼ï¼Œçœå¾—è¿™å°ç˜ªçŠŠå­å‘å’±
            return term.replace('"', '').replace("'", "").replace("`", "").strip()
        except:
            return text

    def normalize_input(self, text, context_type="EMPTY_CHECK"):
        """
        è§„èŒƒåŒ–ç”¨æˆ·è¾“å…¥ã€‚
        context_type: EMPTY_CHECK (è¡¥å…¨å­—æ®µ), SELECTION (é€‰æ‹©é¢˜)
        è¿”å›: è§„èŒƒåŒ–åçš„å­—ç¬¦ä¸²ï¼Œå¦‚æœæ˜¯æ— æ•ˆè¾“å…¥åˆ™è¿”å› ""
        """
        if not text or not text.strip(): return ""
        
        from volcenginesdkarkruntime import Ark
        client = Ark(api_key=self.api_key)
        prompt_path = Path("config/prompts/normalize_input.txt")
        if not prompt_path.exists(): return text
        
        with open(prompt_path, "r", encoding="utf-8") as f:
            sys_prompt = f.read()
            
        user_msg = f"Context Type: {context_type}\nUser Input: {text}"

        try:
            completion = client.chat.completions.create(
                model=self.endpoint_id,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_msg},
                ],
                temperature=0.1,
            )
            normalized = completion.choices[0].message.content.strip()
            if "[[NULL]]" in normalized:
                return ""
            return normalized
        except:
            return text

    def _get_safe_filename(self, project_name):
        """å°†é¡¹ç›®åè½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å"""
        # æ›¿æ¢æ‰ / \ : * ? " < > | ç­‰éæ³•å­—ç¬¦
        safe_name = re.sub(r'[\\/:*?"<>|]', '_', project_name)
        return self.data_dir / f"{safe_name}.json"

    def list_opportunities(self, filter_func=None):
        """
        è·å–ç¬¦åˆæ¡ä»¶çš„å•†æœºåˆ—è¡¨ (List æ“ä½œ)
        filter_func: ä¸€ä¸ªå‡½æ•°ï¼Œæ¥æ”¶å•†æœºæ•°æ® dictï¼Œè¿”å› boolã€‚å¦‚æœä¸ä¼ åˆ™è¿”å›æ‰€æœ‰ã€‚
        """
        all_data = self.get_all_opportunities()
        if not filter_func:
            return all_data
        
        filtered = []
        for item in all_data:
            if filter_func(item):
                filtered.append(item)
        return filtered

    def search_opportunities(self, keyword):
        """æ ¹æ®å…³é”®å­—æ¨¡ç³Šæœç´¢å·²æœ‰çš„é¡¹ç›®åç§°ï¼Œè¿”å› (é¡¹ç›®å, é”€å”®) åˆ—è¡¨ã€‚"""
        # ä½¿ç”¨ list_opportunities å®ç°
        def keyword_filter(data):
            p_name = data.get("project_opportunity", {}).get("project_name", "")
            if not p_name: p_name = data.get("project_name", "")
            return keyword.lower() in p_name.lower()
            
        matches = []
        for p in self.list_opportunities(keyword_filter):
            p_name = p.get("project_opportunity", {}).get("project_name", "")
            if not p_name: p_name = p.get("project_name", "")
            matches.append({
                "name": p_name,
                "sales_rep": p.get("sales_rep", "æœªçŸ¥")
            })
        return matches

    def find_potential_matches(self, project_name):
        """
        ç»“åˆå…³é”®å­—å’Œå‘é‡æœç´¢ï¼Œå¯»æ‰¾ç–‘ä¼¼å­˜åœ¨çš„é¡¹ç›®ã€‚
        è¿”å›: [{"name": "é¡¹ç›®A", "source": "keyword/vector", "id": "..."}]
        """
        candidates = {} # ç”¨ name åš key å»é‡

        # 1. å…³é”®å­—æœç´¢ (æœ¬åœ°æ–‡ä»¶æ‰«æ)
        kw_matches = self.search_opportunities(project_name)
        for m in kw_matches:
            candidates[m["name"]] = {"name": m["name"], "source": "å…³é”®å­—åŒ¹é…", "sales_rep": m["sales_rep"]}

        # 2. å‘é‡æœç´¢ (è¯­ä¹‰è¿‘ä¼¼)
        if self.vector_service:
            vec_matches = self.vector_service.search_projects(project_name)
            for vm in vec_matches:
                p_name = vm["project_name"]
                # åªæœ‰å½“å…³é”®å­—æ²¡æœåˆ°ï¼Œä¸”åå­—ä¸å®Œå…¨ä¸€æ ·æ—¶æ‰åŠ è¿›å»ï¼ˆé¿å…é‡å¤ï¼‰
                if p_name not in candidates:
                    candidates[p_name] = {"name": p_name, "source": "è¯­ä¹‰ç›¸ä¼¼", "sales_rep": "æœªçŸ¥"} # å‘é‡åº“æš‚æœªè¿”å›sales_repï¼Œç®€åŒ–å¤„ç†

        return list(candidates.values())

    def handle_query(self, query_text):
        if not self.validate_llm_config():
            return "__ERROR_CONFIG__"
            
        if self.vector_service:
            history = self.vector_service.search(query_text, top_k=5)
        else:
            # å›é€€ï¼šè¯»å–æœ€è¿‘ä¿®æ”¹çš„ 10 ä¸ªæ–‡ä»¶
            history = []
            files = sorted(self.data_dir.glob("*.json"), key=os.path.getmtime, reverse=True)[:10]
            for fp in files:
                try:
                    with open(fp, "r", encoding="utf-8") as f:
                        history.append(json.load(f))
                except: pass
        
        if not history:
            return "__EMPTY_DB__"
            
        return query_sales_data(query_text, history, self.api_key, self.endpoint_id)

    def get_missing_fields(self, data):
        if "project_opportunity" not in data:
            data["project_opportunity"] = {}

        required_config = {
            "sales_rep": ("ğŸ‘¨â€ğŸ’¼ æˆ‘æ–¹é”€å”®", None),
            "opportunity_stage": ("ğŸ“ˆ å•†æœºé˜¶æ®µ (1:éœ€æ±‚ç¡®è®¤ 2:æ²Ÿé€šäº¤æµ 3:å•†åŠ¡è°ˆåˆ¤ 4:ç­¾è®¢åˆåŒ)", "project_opportunity"),
            "timeline": ("â±ï¸ æ—¶é—´èŠ‚ç‚¹", "project_opportunity"),
            "budget": ("ğŸ’° é¢„ç®—é‡‘é¢", "project_opportunity"),
            "procurement_process": ("ğŸ“ é‡‡è´­æµç¨‹", "project_opportunity"),
            "competitors": ("âš”ï¸ ç«äº‰å¯¹æ‰‹", "project_opportunity"),
            "technical_staff": ("ğŸ§‘â€ğŸ’» æˆ‘æ–¹æŠ€æœ¯äººå‘˜", "project_opportunity"),
            "payment_terms": ("ğŸ’³ ä»˜æ¬¾æ–¹å¼", "project_opportunity")
        }
        
        missing = {}
        for field_key, (field_name, parent_key) in required_config.items():
            target_dict = data.get(parent_key) if parent_key else data
            val = target_dict.get(field_key) if target_dict else None
            is_missing = False
            if val is None: is_missing = True
            elif isinstance(val, str) and (not val.strip() or val in ["æœªçŸ¥", "æœªæŒ‡å®š", "N/A"]): is_missing = True
            elif isinstance(val, list) and not val: is_missing = True
            
            if is_missing:
                missing[field_key] = (field_name, parent_key)
        return missing

    def refine(self, data, supplements):
        return refine_sales_data(data, supplements, self.api_key, self.endpoint_id)

    def update(self, data, instruction):
        updated_data = update_sales_data(data, instruction, self.api_key, self.endpoint_id)
        
        # --- å¼ºä¸€è‡´æ€§åŒæ­¥é€»è¾‘ ---
        # ç¡®ä¿ project_name åœ¨å„å¤„ä¿æŒä¸€è‡´
        new_opp = updated_data.get("project_opportunity", {})
        inner_name = new_opp.get("project_name")
        outer_name = updated_data.get("project_name")
        
        # å¦‚æœå†…éƒ¨æ”¹äº†ï¼ŒåŒæ­¥åˆ°å¤–éƒ¨
        if inner_name and inner_name != outer_name:
            updated_data["project_name"] = inner_name
        # å¦‚æœå¤–éƒ¨æ”¹äº†ï¼ˆä¸”å†…éƒ¨æ²¡æ”¹æˆ–ä¸ºç©ºï¼‰ï¼ŒåŒæ­¥åˆ°å†…éƒ¨
        elif outer_name and outer_name != inner_name:
            if "project_opportunity" not in updated_data: updated_data["project_opportunity"] = {}
            updated_data["project_opportunity"]["project_name"] = outer_name
            
        # --- [å…³é”®ä¿®å¤]ï¼šä¿ç•™ç³»ç»Ÿçº§å…ƒæ•°æ® ---
        # LLM è¿”å›çš„æ•°æ®é‡Œæ²¡æœ‰è¿™äº›ç§æœ‰å­—æ®µï¼Œå¿…é¡»ä»åŸæ•°æ®æ‹·è¿‡æ¥ï¼
        meta_keys = ["id", "_file_path", "_temp_id", "created_at", "record_logs", "updated_at"]
        for k in meta_keys:
            if k in data and k not in updated_data:
                updated_data[k] = data[k]
            
        return updated_data

    def save(self, record, raw_content=""):
        """
        ä¿å­˜å•†æœºä¿¡æ¯ï¼šæ¯ä¸ªå•†æœºä¸€ä¸ªç‹¬ç«‹ JSON æ–‡ä»¶ã€‚
        raw_content: polish_text.txt æ¶¦è‰²åçš„åŸå§‹æ–‡å­—ã€‚
        """
        # 1. å‡†å¤‡å†…å®¹ï¼šäºŒé€‰ä¸€åŸåˆ™
        polished_text = raw_content if raw_content else record.get("summary", "")
        
        # æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœæ¶¦è‰²æ–‡æœ¬ > 500å­—ï¼Œåˆ™ç”Ÿæˆæ‘˜è¦ä½œä¸ºæœ€ç»ˆå†…å®¹ï¼›å¦åˆ™ç›´æ¥ç”¨æ¶¦è‰²æ–‡æœ¬
        if len(polished_text) > 500:
            final_content = summarize_text(polished_text, self.api_key, self.endpoint_id)
        else:
            final_content = polished_text
        
        now = datetime.datetime.now()
        proj_info = record.get("project_opportunity", {})
        proj_name = proj_info.get("project_name", "æœªå‘½åé¡¹ç›®")
        
        # 2. ç¡®å®šæ–‡ä»¶è·¯å¾„
        file_path = self._get_safe_filename(proj_name)
        
        # 3. è¯»å–ç°æœ‰æ–‡ä»¶æˆ–åˆ›å»ºæ–°ç»“æ„
        if file_path.exists():
            with open(file_path, "r", encoding="utf-8") as f:
                try: target_proj = json.load(f)
                except: target_proj = {}
            is_new = False
        else:
            target_proj = {
                "id": str(int(datetime.datetime.now().timestamp())), # ä¸´æ—¶ç”Ÿæˆå”¯ä¸€ID
                "created_at": now.isoformat(),
                "record_logs": []
            }
            is_new = True

        # 4. æ„é€ æœ¬æ¬¡è®°å½•çš„å°è®° (å¿…é¡»åŒ…å«æ—¶é—´ã€è®°å½•è€…)
        new_log_entry = {
            "time": now.strftime("%Y-%m-%d %H:%M:%S"),
            "recorder": self.default_recorder,
            "content": final_content  # æœ€ç»ˆå…¥åº“å†…å®¹ï¼ˆæ‘˜è¦æˆ–åŸè¯ï¼‰
        }

        # 5. æ›´æ–°æ•°æ®
        target_proj.update(record) 
        if "project_opportunity" not in target_proj: target_proj["project_opportunity"] = {}
        target_proj["project_opportunity"].update(proj_info)
        
        if "customer_info" not in target_proj: target_proj["customer_info"] = {}
        target_proj["customer_info"].update(record.get("customer_info", {}))
        
        if "record_logs" not in target_proj: target_proj["record_logs"] = []
        target_proj["record_logs"].append(new_log_entry)
        
        target_proj["updated_at"] = now.isoformat()
        
        # 6. å†™å›æ–‡ä»¶
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(target_proj, f, ensure_ascii=False, indent=2)
            
        record_id = target_proj.get("id")

        # 7. å‘é‡åº“åŒæ­¥ (ä½¿ç”¨ upsert ç¡®ä¿æ˜¯æœ€æ–°çš„)
        if self.vector_service:
            try:
                self.vector_service.add_record(record_id, target_proj)
            except: pass
            
        return record_id, str(file_path)

    def get_all_opportunities(self):
        """è·å–æ‰€æœ‰å•†æœºè®°å½• (æ‰«æ data/opportunities ç›®å½•)"""
        all_data = []
        # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åˆ—
        files = sorted(self.data_dir.glob("*.json"), key=os.path.getmtime, reverse=True)
        
        for idx, fp in enumerate(files):
            try:
                with open(fp, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    # åŠ¨æ€æ³¨å…¥ä¸€ä¸ªä¸´æ—¶ IDï¼Œæ–¹ä¾¿ CLI åˆ—è¡¨é€‰æ‹© (1, 2, 3...)
                    # æ³¨æ„ï¼šè¿™ä¸ª temp_id ä»…ç”¨äºå½“å‰ session çš„æ˜¾ç¤ºï¼Œä¸åšæŒä¹…åŒ–
                    data["_temp_id"] = str(idx + 1)
                    data["_file_path"] = str(fp)
                    all_data.append(data)
            except: pass
        return all_data

    def get_opportunity_by_id(self, record_id):
        """
        æ ¹æ® ID è·å–å•æ¡å•†æœºã€‚
        ç”±äºç°åœ¨æ˜¯æ–‡ä»¶å­˜å‚¨ï¼Œä¸” ID å¯èƒ½æ˜¯æŒä¹…åŒ–çš„ timestamp IDï¼Œä¹Ÿå¯èƒ½æ˜¯ CLI çš„ä¸´æ—¶ IDã€‚
        è¿™é‡Œåšä¸€ä¸ªå…¼å®¹é€»è¾‘ã€‚
        """
        all_data = self.get_all_opportunities()
        
        # 1. å…ˆå°è¯•åŒ¹é… _temp_id (CLI è¾“å…¥çš„ 1, 2, 3)
        for item in all_data:
            if str(item.get("_temp_id")) == str(record_id):
                return item
        
        # 2. å¦‚æœæ²¡åŒ¹é…åˆ°ï¼Œå°è¯•åŒ¹é…çœŸå®çš„ id
        for item in all_data:
            if str(item.get("id")) == str(record_id):
                return item
                
        return None

    def delete_opportunity(self, record_id):
        """æ ¹æ® ID åˆ é™¤å•†æœº"""
        target = self.get_opportunity_by_id(record_id)
        if not target: return False
        
        file_path = Path(target.get("_file_path", ""))
        real_id = target.get("id")
        
        if file_path.exists():
            try:
                os.remove(file_path)
                # å‘é‡åº“åˆ é™¤
                if self.vector_service and real_id:
                    self.vector_service.delete_record(real_id)
                return True
            except Exception as e:
                print(f"Delete error: {{e}}")
                return False
        return False

    def overwrite_opportunity(self, new_data):
        """
        è¦†ç›–æ›´æ–°å•†æœº (ç¼–è¾‘æ¨¡å¼)
        å¤„ç†é‡å‘½åé€»è¾‘ï¼šå¦‚æœé¡¹ç›®åå˜äº†ï¼Œæ–‡ä»¶åä¹Ÿå¾—è·Ÿç€å˜ã€‚
        """
        old_file_path_str = new_data.get("_file_path")
        proj_name = new_data.get("project_opportunity", {}).get("project_name")
        if not proj_name: 
            return False
            
        new_file_path = self._get_safe_filename(proj_name)
        
        # æ¸…ç†ä¸´æ—¶å­—æ®µ
        save_data = new_data.copy()
        save_data.pop("_temp_id", None)
        save_data.pop("_file_path", None)
        
        save_data["updated_at"] = datetime.datetime.now().isoformat()
        
        try:
            # 1. å†™å…¥æ–°æ–‡ä»¶
            with open(new_file_path, "w", encoding="utf-8") as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            # 2. å¦‚æœæ–‡ä»¶åå˜äº†ï¼ŒæŠŠæ—§æ–‡ä»¶åˆ äº†
            if old_file_path_str and Path(old_file_path_str).exists():
                old_file_path = Path(old_file_path_str)
                if old_file_path.resolve() != new_file_path.resolve():
                    os.remove(old_file_path)
            
            # 3. å‘é‡åº“åŒæ­¥ (ID ä¿æŒä¸å˜ï¼Œå†…å®¹è¦†ç›–)
            if self.vector_service:
                self.vector_service.add_record(save_data.get("id"), save_data)
            return True
        except Exception as e:
            print(f"Update error: {e}")
            return False

    def judge_user_affirmative(self, text):
        """
        é€šç”¨å¸ƒå°”æ„å›¾åˆ¤æ–­å™¨ã€‚
        åˆ¤æ–­ç”¨æˆ·æ˜¯å¦è¡¨è¾¾äº†ã€è‚¯å®š/åŒæ„/ç¡®è®¤ã€‘çš„æ€åº¦ã€‚
        """
        if not text: return False
        t = text.strip().lower()
        
        # 1. å¦å®šå¿«ç­› (Fast Negative) - åªè¦æ²¾è¾¹å°±æ‹’ï¼Œçœæµ
        neg_kw = ["ä¸", "å¦", "åˆ«", "å–æ¶ˆ", "no", "n", "cancel", "ç®—äº†", "æ”¾å¼ƒ"]
        # å¦‚æœæ˜¯çŸ­è¯­ä¸”åŒ…å«å¦å®šè¯ï¼Œç›´æ¥ False
        if len(t) < 10 and any(k in t for k in neg_kw):
            return False

        # 2. è‚¯å®šå¿«ç­› (Fast Positive)
        aff_kw = ["æ˜¯", "æ”¹", "è¦†ç›–", "å¯¹", "yes", "y", "ok", "ç¡®è®¤", "å¥½", "å—¯", "å¦¥", "å­˜", "å…³è”", "æ–°å»º"]
        if len(t) < 10 and any(k in t for k in aff_kw):
            return True

        # 3. å¤æ‚æƒ…å†µäº¤ç»™ LLM (AI ä»²è£)
        from src.services.llm_service import judge_affirmative
        return judge_affirmative(text, self.api_key, self.endpoint_id)

    def detect_data_conflicts(self, old_data, new_data):
        """
        å¯¹æ¯”æ–°æ—§æ•°æ®ï¼Œæ£€æµ‹å­—æ®µå†²çªã€‚
        è¿”å›: list of (category, key, field_name, old_val, new_val)
        """
        conflicts = []
        
        # å­—æ®µåç§°æ˜ å°„ (ç”¨äºæ˜¾ç¤º)
        field_labels = {
            "budget": "é¢„ç®—é‡‘é¢",
            "opportunity_stage": "å•†æœºé˜¶æ®µ",
            "timeline": "æ—¶é—´èŠ‚ç‚¹",
            "procurement_process": "é‡‡è´­æµç¨‹",
            "payment_terms": "ä»˜æ¬¾æ–¹å¼",
            "name": "å®¢æˆ·å§“å",
            "company": "å®¢æˆ·å…¬å¸",
            "role": "å®¢æˆ·èŒä½",
            "contact": "è”ç³»æ–¹å¼"
        }

        # 1. æ£€æŸ¥ project_opportunity
        old_opp = old_data.get("project_opportunity", {})
        new_opp = new_data.get("project_opportunity", {})
        
        for k, v_new in new_opp.items():
            # å¿½ç•¥éä¸šåŠ¡å­—æ®µ
            if k in ["is_new_project", "project_name"]: continue
            # å¦‚æœæ–°å€¼ä¸ºç©ºï¼Œä¸è§†ä¸ºå†²çªï¼ˆä¸è¦†ç›–ï¼‰
            if not v_new or v_new == "null": continue
            
            v_old = old_opp.get(k)
            # è½¬å­—ç¬¦ä¸²æ¯”è¾ƒï¼Œå¿½ç•¥ç±»å‹å·®å¼‚
            if str(v_new) != str(v_old) and v_old:
                label = field_labels.get(k, k)
                conflicts.append(("project_opportunity", k, label, v_old, v_new))

        # 2. æ£€æŸ¥ customer_info
        old_cust = old_data.get("customer_info", {})
        new_cust = new_data.get("customer_info", {})
        
        for k, v_new in new_cust.items():
            if not v_new: continue
            v_old = old_cust.get(k)
            if str(v_new) != str(v_old) and v_old:
                label = field_labels.get(k, k)
                conflicts.append(("customer_info", k, label, v_old, v_new))

        return conflicts