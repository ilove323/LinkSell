import configparser
import json
import datetime
import re
import os
import glob
from pathlib import Path
from src.services.llm_service import (
    polish_text, classify_intent, query_sales_data, summarize_text,
    architect_analyze
)
from src.services.asr_service import transcribe_audio
from src.services.vector_service import VectorService

class LinkSellController:
    def __init__(self, config_path="config/config.ini"):
        self.config = configparser.ConfigParser()
        self.config_path = Path(config_path)
        if self.config_path.exists():
            self.config.read(self.config_path)
        
        # 1. è®¾ç½®å…¨å±€ç¯å¢ƒå˜é‡ä¸é»˜è®¤è®°å½•è€…
        hf_endpoint = self.config.get("global", "hf_endpoint", fallback=None)
        if hf_endpoint:
            os.environ["HF_ENDPOINT"] = hf_endpoint
        
        self.default_recorder = self.config.get("global", "default_recorder", fallback="é™ˆä¸€éª")
        self.note_buffer = [] # ç¬”è®°æš‚å­˜åŒº (V3.0)
            
        self.api_key = self.config.get("doubao", "api_key", fallback=None)
        self.endpoint_id = self.config.get("doubao", "analyze_endpoint", fallback=None)
        
        # ASR Config
        self.asr_app_id = self.config.get("asr", "app_id", fallback=None)
        self.asr_token = self.config.get("asr", "access_token", fallback=None)
        self.asr_resource = self.config.get("asr", "resource_id", fallback="volc.seedasr.auc")
        if self.asr_resource == "volc.bigasr.sauc.duration":
             self.asr_resource = "volc.seedasr.auc"

        # 2. åŠ è½½å•†æœºé˜¶æ®µæ˜ å°„
        self.stage_map = {}
        if self.config.has_section("opportunity_stages"):
            self.stage_map = {k: v for k, v in self.config.items("opportunity_stages")}

        # 3. åˆå§‹åŒ–æ•°æ®ç›®å½•
        self.data_dir = Path("data/opportunities")
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # 4. åˆå§‹åŒ–æœ¬åœ°å‘é‡åº“
        try:
            self.vector_service = VectorService()
        except Exception as e:
            print(f"[yellow]è­¦å‘Šï¼šæœ¬åœ°å‘é‡æ¨¡å‹åŠ è½½å¤±è´¥({{e}})é”›ï¿½å°†å›é€€åˆ°æ™®é€šæŸ¥è¯¢æ¨¡å¼éŠ†ï¿½[/yellow]")
            self.vector_service = None

    def validate_llm_config(self):
        return bool(self.api_key and self.endpoint_id and "YOUR_" not in self.api_key)

    def validate_asr_config(self):
        return bool(self.asr_app_id and self.asr_token and "YOUR_" not in self.asr_token)

    def transcribe(self, audio_file, debug=False):
        if not self.validate_asr_config():
            raise ValueError("ASR Configuration Invalid")
        return transcribe_audio(audio_file, self.asr_app_id, self.asr_token, self.asr_resource, debug=debug)

    def polish(self, text):
        if not self.validate_llm_config():
            raise ValueError("LLM Configuration Invalid")
        return polish_text(text, self.api_key, self.endpoint_id)

    def identify_intent(self, text):
        """è¯†åˆ«æ„å›¾å’Œå†…å®¹ï¼Œè¿”å› {"intent": "...", "content": "..."}"""
        if not self.validate_llm_config():
            return {"intent": "RECORD", "content": text}
        
        # è°ƒç”¨ LLM è¿›è¡Œåˆ†ç±»ï¼ŒæœŸæœ›è¿”å› JSON æ ¼å¼
        result = classify_intent(text, self.api_key, self.endpoint_id)
        
        # å°è¯•è§£æ JSON å“åº”
        try:
            if isinstance(result, dict):
                parsed = result
            else:
                # å¦‚æœ LLM è¿”å›å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
                parsed = json.loads(result) if isinstance(result, str) else {"intent": result}
            
            intent = parsed.get("intent", "RECORD").upper()
            content = parsed.get("content", text)
        except:
            # JSON è§£æå¤±è´¥ï¼Œé™çº§ä¸ºå…³é”®è¯åˆ¤æ–­ (V3.0 ç½®æ¢ç‰ˆ)
            intent = "RECORD" # é»˜è®¤å½’ä¸ºç¬”è®°æš‚å­˜
            content = text
            if any(k in text for k in ["ä¿å­˜"]): 
                intent = "SAVE"
            elif any(k in text for k in ["æ­£å¼ä¿å­˜", "æ­£å¼å½•å…¥", "æäº¤åˆ°", "åˆ›å»ºé¡¹ç›®", "æ–°å»ºé¡¹ç›®", "å­˜å…¥å•†æœº"]): 
                intent = "CREATE"
            elif any(k in text for k in ["æŸ¥", "æ‰¾", "çœ‹", "å“ªäº›", "æœç´¢", "åˆ—è¡¨"]): 
                intent = "LIST"
            elif any(k in text for k in ["åˆ ", "ç§»é™¤"]): 
                intent = "DELETE"
            elif any(k in text for k in ["æ”¹", "æ›´æ–°", "æ¢"]): 
                intent = "REPLACE"
        
        # ä¸¥æ ¼è§„èŒƒåŒ–æ„å›¾
        valid_intents = ["CREATE", "LIST", "GET", "REPLACE", "DELETE", "RECORD", "SAVE", "MERGE", "OTHER"]
        if intent not in valid_intents:
            intent = "RECORD"
        
        # OTHER çš„äººå·¥å¤æ ¸ï¼šé˜²æ­¢å¯¹ä¸šåŠ¡æŒ‡ä»¤çš„è¯¯æ€ (V3.0 ç½®æ¢ç‰ˆ)
        if intent == "OTHER":
            biz_keywords = ["é¡¹ç›®", "å•†æœº", "å•å­", "å®¢æˆ·", "èŠ", "è°ˆ", "é¢„ç®—", "è¿›åº¦", "è·Ÿè¿›", "è¯¦æƒ…", "æ¡£æ¡ˆ", "ä¼šè®®", "ä¸€æœŸ", "äºŒæœŸ"]
            if len(text) > 8 or any(k in text for k in biz_keywords):
                intent = "RECORD"
        
        return {"intent": intent, "content": content}

    def extract_search_term(self, text):
        """
        ä½¿ç”¨ LLM ä»ç”¨æˆ·æŒ‡ä»¤ä¸­æå–æ ¸å¿ƒæœç´¢è¯ï¼ˆé¡¹ç›®åï¼‰ã€‚
        ä¾‹å¦‚ï¼š"æŸ¥çœ‹æ²ˆé˜³è½´æ‰¿å‚è¯¦æƒ…" -> "æ²ˆé˜³è½´æ‰¿å‚"
        """
        from volcenginesdkarkruntime import Ark
        
        client = Ark(api_key=self.api_key)
        # åŠ è½½ prompt
        prompt_path = Path("config/prompts/extract_search_term.txt")
        if not prompt_path.exists(): return text # Fallback
        
        with open(prompt_path, "r", encoding="utf-8") as f:
            sys_prompt = f.read()

        try:
            completion = client.chat.completions.create(
                model=self.endpoint_id,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": text},
                ],
                temperature=0.1, 
            )
            term = completion.choices[0].message.content.strip()
            if "Unknown" in term: return text
            # å»é™¤å¯èƒ½çš„å¼•å·å’Œåå¼•å·ï¼Œå¹¶ä¸”å‰åå»ç©ºæ ¼ï¼Œçœå¾—è¿™å°ç˜ªçŠŠå­å‘å’±
            return term.replace('"', '').replace("'", "").replace("`", "").strip()
        except:
            return text

    def normalize_input(self, text, context_type="EMPTY_CHECK"):
        """
        è§„èŒƒåŒ–ç”¨æˆ·è¾“å…¥ã€‚
        context_type: EMPTY_CHECK (è¡¥å…¨å­—æ®µ), SELECTION (é€‰æ‹©é¢˜)
        è¿”å›: è§„èŒƒåŒ–åçš„å­—ç¬¦ä¸²ï¼Œå¦‚æœæ˜¯æ— æ•ˆè¾“å…¥åˆ™è¿”å› ""
        """
        if not text or not text.strip(): return ""
        
        from volcenginesdkarkruntime import Ark
        client = Ark(api_key=self.api_key)
        prompt_path = Path("config/prompts/normalize_input.txt")
        if not prompt_path.exists(): return text
        
        with open(prompt_path, "r", encoding="utf-8") as f:
            sys_prompt = f.read()
            
        user_msg = f"Context Type: {context_type}\nUser Input: {text}"

        try:
            completion = client.chat.completions.create(
                model=self.endpoint_id,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_msg},
                ],
                temperature=0.1,
            )
            normalized = completion.choices[0].message.content.strip()
            if "[[NULL]]" in normalized:
                return ""
            return normalized
        except:
            return text

    def _get_safe_filename(self, project_name):
        """å°†é¡¹ç›®åè½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å"""
        # æ›¿æ¢æ‰ / \ : * ? " < > | ç­‰éæ³•å­—ç¬¦
        safe_name = re.sub(r'[\\/:*?"<>|]', '_', project_name)
        return self.data_dir / f"{safe_name}.json"

    def list_opportunities(self, filter_func=None):
        """
        è·å–ç¬¦åˆæ¡ä»¶çš„å•†æœºåˆ—è¡¨ (List æ“ä½œ)
        filter_func: ä¸€ä¸ªå‡½æ•°ï¼Œæ¥æ”¶å•†æœºæ•°æ® dictï¼Œè¿”å› boolã€‚å¦‚æœä¸ä¼ åˆ™è¿”å›æ‰€æœ‰ã€‚
        """
        all_data = self.get_all_opportunities()
        if not filter_func:
            return all_data
        
        filtered = []
        for item in all_data:
            if filter_func(item):
                filtered.append(item)
        return filtered

    def search_opportunities(self, keyword):
        """æ ¹æ®å…³é”®å­—æ¨¡ç³Šæœç´¢å·²æœ‰çš„é¡¹ç›®åç§°ï¼Œè¿”å› (é¡¹ç›®å, é”€å”®) åˆ—è¡¨ã€‚"""
        # ä½¿ç”¨ list_opportunities å®ç°
        def keyword_filter(data):
            p_name = data.get("project_opportunity", {}).get("project_name", "")
            if not p_name: p_name = data.get("project_name", "")
            # åŒå‘åŒ…å«ï¼šæœ "è½´æ‰¿" -> "æ²ˆé˜³è½´æ‰¿" (Keyword in Name)
            #           æœ "æŸ¥çœ‹æ²ˆé˜³è½´æ‰¿" -> "æ²ˆé˜³è½´æ‰¿" (Name in Keyword)
            k_low = keyword.lower(); p_low = p_name.lower()
            return (k_low in p_low) or (len(p_name) > 2 and p_low in k_low)
            
        matches = []
        for p in self.list_opportunities(keyword_filter):
            p_name = p.get("project_opportunity", {}).get("project_name", "")
            if not p_name: p_name = p.get("project_name", "")
            matches.append({
                "name": p_name,
                "sales_rep": p.get("sales_rep", "æœªçŸ¥"),
                "id": p.get("id")
            })
        return matches

    def find_potential_matches(self, project_name):
        """
        ç»“åˆå…³é”®å­—å’Œå‘é‡æœç´¢ï¼Œå¯»æ‰¾ç–‘ä¼¼å­˜åœ¨çš„é¡¹ç›®ã€‚
        è¿”å›: [{"name": "é¡¹ç›®A", "source": "keyword/vector", "id": "..."}]
        """
        candidates = {} # ç”¨ name åš key å»é‡

        # 1. å…³é”®å­—æœç´¢ (æœ¬åœ°æ–‡ä»¶æ‰«æ)
        kw_matches = self.search_opportunities(project_name)
        for m in kw_matches:
            candidates[m["name"]] = {"name": m["name"], "source": "å…³é”®å­—åŒ¹é…", "sales_rep": m["sales_rep"], "id": m["id"]}

        # 2. å‘é‡æœç´¢ (è¯­ä¹‰è¿‘ä¼¼)
        if self.vector_service:
            vec_matches = self.vector_service.search_projects(project_name)
            for vm in vec_matches:
                p_name = vm["project_name"]
                # åªæœ‰å½“å…³é”®å­—æ²¡æœåˆ°ï¼Œä¸”åå­—ä¸å®Œå…¨ä¸€æ ·æ—¶æ‰åŠ è¿›å»ï¼ˆé¿å…é‡å¤ï¼‰
                if p_name not in candidates:
                    candidates[p_name] = {"name": p_name, "source": "è¯­ä¹‰ç›¸ä¼¼", "sales_rep": "æœªçŸ¥", "id": vm.get("id")} # å‘é‡åº“æš‚æœªè¿”å›sales_repï¼Œç®€åŒ–å¤„ç†

        # --- [ä¼˜åŒ–] ç²¾ç¡®åŒ¹é…ä¼˜å…ˆç­–ç•¥ ---
        clean_search = project_name.strip().lower()
        exact_match = None
        contained_match = None
        max_len = 0
        
        for name, cand in candidates.items():
            c_name = name.strip().lower()
            
            # 1. ç»å¯¹ç²¾ç¡®åŒ¹é… (æœ€é«˜ä¼˜å…ˆçº§)
            if c_name == clean_search:
                return [cand]
            
            # 2. åŒ…å«åŒ¹é… (Name in SearchTerm) - å¤„ç†æå–ä¸å¹²å‡€çš„æƒ…å†µ
            # ä¾‹å¦‚ Search="å•†æœºæ²ˆé˜³æœºåºŠ", Name="æ²ˆé˜³æœºåºŠ"
            if len(c_name) > 2 and c_name in clean_search:
                # è´ªå©ªåŒ¹é…ï¼šå¦‚æœæœ‰å¤šä¸ªè¢«åŒ…å«çš„ï¼Œå–åå­—æœ€é•¿çš„é‚£ä¸ª
                # (é˜²æ­¢æœ "AäºŒæœŸ" æ—¶åŒ¹é…åˆ° "A")
                if len(c_name) > max_len:
                    max_len = len(c_name)
                    contained_match = cand
        
        if contained_match:
            return [contained_match]

        return list(candidates.values())

    def handle_query(self, query_text):
        if not self.validate_llm_config():
            return "__ERROR_CONFIG__"
            
        if self.vector_service:
            history = self.vector_service.search(query_text, top_k=5)
        else:
            # å›é€€ï¼šè¯»å–æœ€è¿‘ä¿®æ”¹çš„ 10 ä¸ªæ–‡ä»¶
            history = []
            files = sorted(self.data_dir.glob("*.json"), key=os.path.getmtime, reverse=True)[:10]
            for fp in files:
                try:
                    with open(fp, "r", encoding="utf-8") as f:
                        history.append(json.load(f))
                except: pass
        
        if not history:
            return "__EMPTY_DB__"
            
        return query_sales_data(query_text, history, self.api_key, self.endpoint_id)

    def get_missing_fields(self, data):
        if "project_opportunity" not in data:
            data["project_opportunity"] = {}

        required_config = {
            "sales_rep": ("ğŸ‘¨â€ğŸ’¼ æˆ‘æ–¹é”€å”®", None),
            "opportunity_stage": ("ğŸ“ˆ å•†æœºé˜¶æ®µ (1:éœ€æ±‚ç¡®è®¤ 2:æ²Ÿé€šäº¤æµ 3:å•†åŠ¡è°ˆåˆ¤ 4:ç­¾è®¢åˆåŒ)", "project_opportunity"),
            "timeline": ("â±ï¸ æ—¶é—´èŠ‚ç‚¹", "project_opportunity"),
            "budget": ("ğŸ’° é¢„ç®—é‡‘é¢", "project_opportunity"),
            "procurement_process": ("ğŸ“ é‡‡è´­æµç¨‹", "project_opportunity"),
            "competitors": ("âš”ï¸ ç«äº‰å¯¹æ‰‹", "project_opportunity"),
            "technical_staff": ("ğŸ§‘â€ğŸ’» æˆ‘æ–¹æŠ€æœ¯äººå‘˜", "project_opportunity"),
            "payment_terms": ("ğŸ’³ ä»˜æ¬¾æ–¹å¼", "project_opportunity")
        }
        
        missing = {}
        for field_key, (field_name, parent_key) in required_config.items():
            target_dict = data.get(parent_key) if parent_key else data
            val = target_dict.get(field_key) if target_dict else None
            is_missing = False
            if val is None: is_missing = True
            elif isinstance(val, str) and (not val.strip() or val in ["æœªçŸ¥", "æœªæŒ‡å®š", "N/A"]): is_missing = True
            elif isinstance(val, list) and not val: is_missing = True
            
            if is_missing:
                missing[field_key] = (field_name, parent_key)
        return missing

    def merge(self, data: dict, note_content: str) -> dict:
        """
        MERGE æµç¨‹ï¼šæ™ºèƒ½åˆå¹¶ç¬”è®°åˆ°å•†æœº
        
        é€»è¾‘ï¼š
        1. è§£æç¬”è®°å†…å®¹ï¼Œæå–ç»“æ„åŒ–å­—æ®µ
        2. å¯¹æ¯”åŸå•†æœºï¼Œé€å­—æ®µæ›´æ–°ï¼ˆè‹¥è§£æåéç©ºä¸”ä¸åŸä¸åŒï¼‰
        3. action_items å’Œ key_points æ‰§è¡Œè¿½åŠ ï¼ˆä¸è¦†ç›–ï¼‰
        4. æœ€åæ·»åŠ  record_log è®°å½•æœ¬æ¬¡ç¬”è®°
        
        è¿”å›: æ›´æ–°åçš„å•†æœºæ•°æ®
        """
        from src.services.llm_service import architect_analyze
        import datetime
        now = datetime.datetime.now()
        
        # æ­¥éª¤1ï¼šè§£æç¬”è®°ï¼Œæå–ç»“æ„åŒ–æ•°æ®
        parsed_data = architect_analyze(
            [note_content],
            self.api_key,
            self.endpoint_id,
            original_data=data,
            recorder=self.default_recorder
        )
        
        if not parsed_data:
            # è§£æå¤±è´¥ï¼Œåªæ·»åŠ åˆ° record_logsï¼Œä¸æ›´æ–°å…¶ä»–å­—æ®µ
            if "record_logs" not in data:
                data["record_logs"] = []
            new_log_entry = {
                "time": now.strftime("%Y-%m-%d %H:%M:%S"),
                "recorder": self.default_recorder,
                "content": note_content
            }
            data["record_logs"].append(new_log_entry)
            data["updated_at"] = now.isoformat()
            return data
        
        # æ­¥éª¤2ï¼šå¯¹æ¯”å¹¶æ™ºèƒ½æ›´æ–°å­—æ®µ
        merged = data.copy()
        
        # 2.1 æ›´æ–°é¡¶å±‚å­—æ®µï¼ˆé™¤äº† project_opportunityï¼‰
        merge_fields = [
            "project_name", "summary", "customer_info", 
            "sentiment", "current_log_entry"
        ]
        
        for field in merge_fields:
            if field in parsed_data:
                new_val = parsed_data[field]
                # æ£€æŸ¥ï¼šéç©ºä¸”ä¸åŸæ•°æ®ä¸åŒï¼Œåˆ™æ›´æ–°
                if new_val and new_val != merged.get(field):
                    merged[field] = new_val
        
        # 2.1.1 ç‰¹æ®Šå¤„ç† opportunity_stageï¼ˆå¿…é¡»åœ¨é¡¶å±‚ï¼Œç¡®ä¿ç±»å‹ä¸ºæ•´æ•°ï¼‰
        stage_val = None
        if "opportunity_stage" in parsed_data:
            stage_val = parsed_data["opportunity_stage"]
        elif "project_opportunity" in parsed_data and isinstance(parsed_data.get("project_opportunity"), dict) and "opportunity_stage" in parsed_data["project_opportunity"]:
            # å¤‡é€‰æ–¹æ¡ˆï¼šå¦‚æœåœ¨ project_opportunity ä¸­ï¼Œä¹Ÿæå–å‡ºæ¥
            stage_val = parsed_data["project_opportunity"]["opportunity_stage"]
        
        # å¦‚æœè·å–åˆ°æ–°é˜¶æ®µå€¼ï¼ˆç±»å‹è½¬æ¢ä¸ºæ•´æ•°ï¼‰ï¼Œä¸”ä¸åŒäºåŸå€¼ï¼Œåˆ™æ›´æ–°é¡¶å±‚çš„ opportunity_stage
        if stage_val is not None:
            try:
                # å°è¯•è½¬æ¢ä¸ºæ•´æ•°ï¼ˆå¦‚æœæ˜¯å­—ç¬¦ä¸²"2"ï¼Œè½¬ä¸º2ï¼‰
                if isinstance(stage_val, str):
                    stage_val = int(stage_val)
                current_stage = merged.get("opportunity_stage")
                if stage_val != current_stage:
                    merged["opportunity_stage"] = stage_val
                    # åŒæ—¶æ›´æ–° project_opportunity ä¸­çš„ opportunity_stageï¼ˆå¦‚æœå­˜åœ¨ï¼‰ä»¥ä¿æŒä¸€è‡´æ€§
                    if "project_opportunity" in merged and isinstance(merged["project_opportunity"], dict):
                        merged["project_opportunity"]["opportunity_stage"] = stage_val
            except (ValueError, TypeError):
                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè·³è¿‡è¯¥å­—æ®µ
                pass
        
        # 2.2 æ›´æ–° project_opportunityï¼ˆåµŒå¥—å­—æ®µï¼‰
        if "project_opportunity" in parsed_data:
            if "project_opportunity" not in merged:
                merged["project_opportunity"] = {}
            
            parsed_opp = parsed_data["project_opportunity"]
            current_opp = merged["project_opportunity"]
            
            # å¯¹ project_opportunity ä¸­çš„å­—æ®µè¿›è¡Œæ›´æ–°ï¼ˆé™¤äº† action_items å’Œ key_pointsï¼‰
            opp_merge_fields = [
                "project_name", "budget", "timeline", "procurement_process",
                "payment_terms", "competitors", "technical_staff", "sentiment"
            ]
            
            for field in opp_merge_fields:
                if field in parsed_opp:
                    new_val = parsed_opp[field]
                    if new_val and new_val != current_opp.get(field):
                        current_opp[field] = new_val
            
            # 2.3 ç‰¹æ®Šå¤„ç†ï¼šaction_items å’Œ key_points æ‰§è¡Œè¿½åŠ 
            if "action_items" in parsed_opp and parsed_opp["action_items"]:
                if "action_items" not in current_opp:
                    current_opp["action_items"] = []
                # å»é‡åè¿½åŠ ï¼ˆé¿å…é‡å¤ï¼‰
                existing_items = set(current_opp["action_items"])
                for item in parsed_opp["action_items"]:
                    if item not in existing_items:
                        current_opp["action_items"].append(item)
            
            if "key_points" in parsed_opp and parsed_opp["key_points"]:
                if "key_points" not in current_opp:
                    current_opp["key_points"] = []
                # å»é‡åè¿½åŠ 
                existing_points = set(current_opp["key_points"])
                for point in parsed_opp["key_points"]:
                    if point not in existing_points:
                        current_opp["key_points"].append(point)
        
        # æ­¥éª¤3ï¼šæ·»åŠ  record_log è®°å½•æœ¬æ¬¡ç¬”è®°
        if "record_logs" not in merged:
            merged["record_logs"] = []
        
        new_log_entry = {
            "time": now.strftime("%Y-%m-%d %H:%M:%S"),
            "recorder": self.default_recorder,
            "content": note_content
        }
        merged["record_logs"].append(new_log_entry)
        merged["updated_at"] = now.isoformat()
        
        return merged

    def replace(self, data, instruction):
        """
        REPLACE æµç¨‹ï¼šä¿®æ”¹å•†æœºæ•°æ® (V3.0 ä½¿ç”¨ Architect å¼•æ“)
        """
        # å°†å•æ¡æŒ‡ä»¤è§†ä¸ºä¸€æ¡ç¬”è®°ï¼Œåˆ©ç”¨ Architect çš„åˆå¹¶èƒ½åŠ›
        updated_data = architect_analyze(
            [instruction], 
            self.api_key, 
            self.endpoint_id, 
            original_data=data, 
            recorder=self.default_recorder
        )
        
        if not updated_data:
            return data
            
        # --- å¼ºä¸€è‡´æ€§åŒæ­¥é€»è¾‘ (ä¿ç•™) ---
        new_opp = updated_data.get("project_opportunity", {})
        inner_name = new_opp.get("project_name")
        outer_name = updated_data.get("project_name")
        
        if inner_name and inner_name != outer_name:
            updated_data["project_name"] = inner_name
        elif outer_name and outer_name != inner_name:
            if "project_opportunity" not in updated_data: updated_data["project_opportunity"] = {}
            updated_data["project_opportunity"]["project_name"] = outer_name
            
        # --- ä¿ç•™ç³»ç»Ÿçº§å…ƒæ•°æ® (ä¿ç•™) ---
        meta_keys = ["id", "_file_path", "_temp_id", "created_at", "record_logs", "updated_at", "recorder"]
        for k in meta_keys:
            if k in data and k not in updated_data:
                updated_data[k] = data[k]
        
        # --- ä¸ºå…¼å®¹æ€§ï¼Œç¡®ä¿ sales_rep å­—æ®µä¸ recorder åŒæ­¥ ---
        # ä¼˜å…ˆçº§ï¼šLLMè¿”å›çš„sales_rep > recorderå€¼
        if "sales_rep" not in updated_data:
            # å¦‚æœLLMæ²¡æœ‰æå–sales_repï¼Œä½¿ç”¨recorderå€¼ä¿æŒä¸€è‡´
            if "recorder" in updated_data:
                updated_data["sales_rep"] = updated_data["recorder"]
            elif "recorder" in data:
                updated_data["sales_rep"] = data["recorder"]
        
        # å¦‚æœä¿®æ”¹äº†sales_repï¼Œä¹ŸåŒæ­¥åˆ°recorderï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
        if "sales_rep" in updated_data and "recorder" not in updated_data:
            updated_data["recorder"] = updated_data["sales_rep"]
        
        # --- [åŸå­æ“ä½œ]ï¼šå¦‚æœä¿®æ”¹äº†å•†æœºåç§°ï¼Œå¤„ç†æ–‡ä»¶é‡å‘½å (ä¿ç•™) ---
        old_proj_name = data.get("project_opportunity", {}).get("project_name")
        new_proj_name = updated_data.get("project_opportunity", {}).get("project_name")
        
        if old_proj_name and new_proj_name and old_proj_name != new_proj_name:
            old_file_path = Path(data.get("_file_path", ""))
            new_file_path = self._get_safe_filename(new_proj_name)
            
            if old_file_path.resolve() != new_file_path.resolve():
                try:
                    save_data = updated_data.copy()
                    save_data.pop("_temp_id", None)
                    save_data.pop("_file_path", None)
                    save_data["updated_at"] = datetime.datetime.now().isoformat()
                    
                    new_file_path.parent.mkdir(parents=True, exist_ok=True)
                    with open(new_file_path, "w", encoding="utf-8") as f:
                        json.dump(save_data, f, ensure_ascii=False, indent=2)
                    
                    if old_file_path.exists():
                        os.remove(old_file_path)
                    
                    if self.vector_service:
                        real_id = updated_data.get("id")
                        if real_id:
                            self.vector_service.delete_record(real_id)
                            self.vector_service.add_record(real_id, save_data)
                    
                    updated_data["_file_path"] = str(new_file_path)
                    
                except Exception as e:
                    print(f"âš ï¸ å•†æœºåç§°é‡å‘½åå¤±è´¥: {e}")
            
        return updated_data

    def save(self, record, raw_content=""):
        """
        ä¿å­˜å•†æœºä¿¡æ¯ï¼šæ¯ä¸ªå•†æœºä¸€ä¸ªç‹¬ç«‹ JSON æ–‡ä»¶ã€‚
        V3.0ï¼šä¼˜å…ˆä½¿ç”¨ current_log_entry å­—æ®µä½œä¸ºæ—¥å¿—å†…å®¹ã€‚
        """
        now = datetime.datetime.now()
        
        # 1. ç¡®å®šæ—¥å¿—å†…å®¹
        # ä¼˜å…ˆä» Architect ç”Ÿæˆçš„ current_log_entry ä¸­è·å–
        final_log_content = record.pop("current_log_entry", None)
        
        if not final_log_content:
            # Fallback 1: ä¼ å…¥çš„åŸå§‹æ–‡æœ¬
            polished_text = raw_content if raw_content else record.get("summary", "")
            # æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œåˆ™ç”Ÿæˆæ‘˜è¦ï¼›å¦åˆ™ç›´æ¥ç”¨
            if polished_text and len(polished_text) > 500:
                final_log_content = summarize_text(polished_text, self.api_key, self.endpoint_id)
            else:
                final_log_content = polished_text or "æ— è¯¦ç»†å°è®°"

        # 2. å‡†å¤‡å°è®°æ¡ç›®
        new_log_entry = {
            "time": now.strftime("%Y-%m-%d %H:%M:%S"),
            "recorder": self.default_recorder,
            "content": final_log_content
        }

        proj_info = record.get("project_opportunity", {})
        proj_name = proj_info.get("project_name", record.get("project_name", "æœªå‘½åé¡¹ç›®"))
        
        # 3. ç¡®å®šæ–‡ä»¶è·¯å¾„
        file_path = self._get_safe_filename(proj_name)
        
        # 4. è¯»å–ç°æœ‰æ–‡ä»¶æˆ–åˆå§‹åŒ–æ–°ç»“æ„
        if file_path.exists():
            with open(file_path, "r", encoding="utf-8") as f:
                try: target_proj = json.load(f)
                except: target_proj = {}
        else:
            target_proj = {
                "id": record.get("id") or str(int(now.timestamp())),
                "created_at": now.isoformat(),
                "record_logs": []
            }

        # 5. æ›´æ–°æ ¸å¿ƒæ•°æ®
        # æ’é™¤æ‰ä¸éœ€è¦åœ¨æŒä¹…åŒ– JSON ä¸­é‡å¤å‡ºç°çš„å…ƒæ•°æ®
        record.pop("_temp_id", None)
        record.pop("_file_path", None)
        
        target_proj.update(record) 
        if "project_opportunity" not in target_proj: target_proj["project_opportunity"] = {}
        target_proj["project_opportunity"].update(proj_info)
        
        if "customer_info" not in target_proj: target_proj["customer_info"] = {}
        target_proj["customer_info"].update(record.get("customer_info", {}))
        
        # 6. è¿½åŠ æ—¥å¿—
        if "record_logs" not in target_proj: target_proj["record_logs"] = []
        target_proj["record_logs"].append(new_log_entry)
        
        target_proj["updated_at"] = now.isoformat()
        
        # 7. å†™å›æ–‡ä»¶
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(target_proj, f, ensure_ascii=False, indent=2)
            
        record_id = target_proj.get("id")

        # 8. å‘é‡åº“åŒæ­¥
        if self.vector_service:
            try:
                self.vector_service.add_record(record_id, target_proj)
            except: pass
            
        return record_id, str(file_path)

    def get_all_opportunities(self):
        """è·å–æ‰€æœ‰å•†æœºè®°å½• (æ‰«æ data/opportunities ç›®å½•)"""
        all_data = []
        # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åˆ—
        files = sorted(self.data_dir.glob("*.json"), key=os.path.getmtime, reverse=True)
        
        for idx, fp in enumerate(files):
            try:
                with open(fp, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    # åŠ¨æ€æ³¨å…¥ä¸€ä¸ªä¸´æ—¶ IDï¼Œæ–¹ä¾¿ CLI åˆ—è¡¨é€‰æ‹© (1, 2, 3...)
                    # æ³¨æ„ï¼šè¿™ä¸ª temp_id ä»…ç”¨äºå½“å‰ session çš„æ˜¾ç¤ºï¼Œä¸åšæŒä¹…åŒ–
                    data["_temp_id"] = str(idx + 1)
                    data["_file_path"] = str(fp)
                    all_data.append(data)
            except: pass
        return all_data

    def get_opportunity_by_id(self, record_id):
        """
        æ ¹æ® ID è·å–å•æ¡å•†æœºã€‚
        ç”±äºç°åœ¨æ˜¯æ–‡ä»¶å­˜å‚¨ï¼Œä¸” ID å¯èƒ½æ˜¯æŒä¹…åŒ–çš„ timestamp IDï¼Œä¹Ÿå¯èƒ½æ˜¯ CLI çš„ä¸´æ—¶ IDã€‚
        è¿™é‡Œåšä¸€ä¸ªå…¼å®¹é€»è¾‘ã€‚
        """
        all_data = self.get_all_opportunities()
        
        # 1. å…ˆå°è¯•åŒ¹é… _temp_id (CLI è¾“å…¥çš„ 1, 2, 3)
        for item in all_data:
            if str(item.get("_temp_id")) == str(record_id):
                return item
        
        # 2. å¦‚æœæ²¡åŒ¹é…åˆ°ï¼Œå°è¯•åŒ¹é…çœŸå®çš„ id
        for item in all_data:
            if str(item.get("id")) == str(record_id):
                return item
                
        return None

    def query(self, sales_data, query_text: str):
        """
        GET/LIST: æ ¹æ®é”€å”®æ•°æ®å’ŒæŸ¥è¯¢é—®é¢˜ï¼Œç”Ÿæˆä¸“ä¸šå›ç­”
        
        Args:
            sales_data: å•ä¸ªå•†æœº JSON æˆ–å•†æœºåˆ—è¡¨
            query_text: ç”¨æˆ·çš„æŸ¥è¯¢é—®é¢˜ï¼ˆä» extracted_content è·å–ï¼‰
            
        Returns:
            çº¯æ–‡æœ¬ç­”æ¡ˆ
        """
        if not self.validate_llm_config():
            return "é…ç½®é”™è¯¯ï¼Œæ— æ³•æŸ¥è¯¢ã€‚"
        
        # å°† sales_data è½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆä¾¿äºç»Ÿä¸€å¤„ç†ï¼‰
        if isinstance(sales_data, dict):
            history_data = [sales_data]
        else:
            history_data = sales_data if isinstance(sales_data, list) else []
        
        return query_sales_data(query_text, history_data, self.api_key, self.endpoint_id)

    def generate_delete_warning(self, sales_data: dict) -> str:
        """
        DELETE: åœ¨åˆ é™¤å‰ç”Ÿæˆå‹å¥½ä½†ä¸¥è‚ƒçš„äºŒæ¬¡ç¡®è®¤æç¤º
        
        Args:
            sales_data: è¦åˆ é™¤çš„å•†æœº JSON
            
        Returns:
            çº¯æ–‡æœ¬çš„åˆ é™¤ç¡®è®¤æç¤º
        """
        if not self.validate_llm_config():
            # fallback: ç›´æ¥è¿”å›åŸºç¡€æç¤º
            proj_name = sales_data.get("project_opportunity", {}).get("project_name", "è¯¥å•†æœº")
            return f"æ‚¨ç¡®å®šè¦åˆ é™¤å•†æœº **{proj_name}** å—ï¼Ÿ\nâš ï¸ æ­¤æ“ä½œä¸å¯é€†ï¼Œè¯·è°¨æ…ç¡®è®¤ã€‚"
        
        # è°ƒç”¨ LLM ç”Ÿæˆæ›´è‡ªç„¶çš„ç¡®è®¤æç¤º
        from volcenginesdkarkruntime import Ark
        from src.services.llm_service import load_prompt
        
        client = Ark(api_key=self.api_key)
        system_prompt = load_prompt("delete_confirmation")
        
        try:
            completion = client.chat.completions.create(
                model=self.endpoint_id,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": json.dumps(sales_data, ensure_ascii=False)},
                ],
                temperature=0.3,
            )
            return completion.choices[0].message.content.strip()
        except Exception as e:
            # å¦‚æœ LLM è°ƒç”¨å¤±è´¥ï¼Œè¿”å›åŸºç¡€æç¤º
            proj_name = sales_data.get("project_opportunity", {}).get("project_name", "è¯¥å•†æœº")
            return f"æ‚¨ç¡®å®šè¦åˆ é™¤å•†æœº **{proj_name}** å—ï¼Ÿ\nâš ï¸ æ­¤æ“ä½œä¸å¯é€†ï¼Œè¯·è°¨æ…ç¡®è®¤ã€‚"

    def delete_opportunity(self, record_id):
        """æ ¹æ® ID åˆ é™¤å•†æœº"""
        target = self.get_opportunity_by_id(record_id)
        if not target: return False
        
        file_path = Path(target.get("_file_path", ""))
        real_id = target.get("id")
        
        if file_path.exists():
            try:
                os.remove(file_path)
                # å‘é‡åº“åˆ é™¤
                if self.vector_service and real_id:
                    self.vector_service.delete_record(real_id)
                return True
            except Exception as e:
                print(f"Delete error: {{e}}")
                return False
        return False

    def merge_draft_into_old(self, old_data: dict, draft_data: dict) -> dict:
        """
        [åˆå¹¶é€»è¾‘] å°†ç¬”è®°è‰ç¨¿åˆå¹¶åˆ°ç°æœ‰å•†æœº
        
        ç”¨äº: ç”¨æˆ·é€šè¿‡ç¬”è®°(RECORD)æœ€åæ‰§è¡ŒCREATEï¼Œç³»ç»Ÿå‘ç°ç¬”è®°æ¶‰åŠç°æœ‰å•†æœºæ—¶è§¦å‘
        æµç¨‹: RECORD â†’ handle_record() â†’ (ç¬”è®°ç§¯ç´¯) â†’ CREATE â†’ handle_create() 
             â†’ process_commit_request() æ£€æµ‹åˆ° linked â†’ è°ƒç”¨æœ¬æ–¹æ³•åˆå¹¶
        
        è¿”å›: åˆå¹¶åçš„å®Œæ•´å•†æœºæ•°æ®
        """
        merged = old_data.copy()
        
        # é€å­—æ®µåˆå¹¶ï¼Œdraft ä¼˜å…ˆçº§é«˜ï¼ˆæ–°æ•°æ®è¦†ç›–æ—§æ•°æ®ï¼‰
        # ä½†ä¿ç•™ old_data çš„å…³é”®å­—æ®µï¼ˆid, _file_path, created_at ç­‰ï¼‰
        for key, value in draft_data.items():
            if key not in ["id", "_file_path", "_temp_id", "created_at"]:
                merged[key] = value
        
        # ç‰¹æ®Šå¤„ç†åµŒå¥—çš„ project_opportunity å­—æ®µ
        if "project_opportunity" in draft_data:
            if "project_opportunity" not in merged:
                merged["project_opportunity"] = {}
            merged["project_opportunity"].update(draft_data["project_opportunity"])
        
        return merged

    def overwrite_opportunity(self, new_data):
        """
        è¦†ç›–æ›´æ–°å•†æœº (ç¼–è¾‘æ¨¡å¼)
        å¤„ç†é‡å‘½åé€»è¾‘ï¼šå¦‚æœé¡¹ç›®åå˜äº†ï¼Œæ–‡ä»¶åä¹Ÿå¾—è·Ÿç€å˜ã€‚
        """
        old_file_path_str = new_data.get("_file_path")
        proj_name = new_data.get("project_opportunity", {}).get("project_name")
        if not proj_name: 
            return False
            
        new_file_path = self._get_safe_filename(proj_name)
        
        # æ¸…ç†ä¸´æ—¶å­—æ®µ
        save_data = new_data.copy()
        save_data.pop("_temp_id", None)
        save_data.pop("_file_path", None)
        
        # --- å…¼å®¹æ€§å¤„ç†ï¼šç¡®ä¿ sales_rep ä¸ recorder åŒæ­¥ ---
        if "sales_rep" in save_data and "recorder" not in save_data:
            # å¦‚æœä¿®æ”¹äº†sales_repï¼Œä¹ŸåŒæ­¥åˆ°recorder
            save_data["recorder"] = save_data["sales_rep"]
        elif "recorder" in save_data and "sales_rep" not in save_data:
            # åå‘ï¼šå¦‚æœæœ‰recorderï¼Œç¡®ä¿sales_repå­˜åœ¨
            save_data["sales_rep"] = save_data["recorder"]
        
        save_data["updated_at"] = datetime.datetime.now().isoformat()
        
        try:
            # 1. å†™å…¥æ–°æ–‡ä»¶
            with open(new_file_path, "w", encoding="utf-8") as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            # 2. å¦‚æœæ–‡ä»¶åå˜äº†ï¼ŒæŠŠæ—§æ–‡ä»¶åˆ äº†
            if old_file_path_str and Path(old_file_path_str).exists():
                old_file_path = Path(old_file_path_str)
                if old_file_path.resolve() != new_file_path.resolve():
                    os.remove(old_file_path)
            
            # 3. å‘é‡åº“åŒæ­¥ (ID ä¿æŒä¸å˜ï¼Œå†…å®¹è¦†ç›–)
            if self.vector_service:
                self.vector_service.add_record(save_data.get("id"), save_data)
            return True
        except Exception as e:
            print(f"Update error: {e}")
            return False

    def judge_user_affirmative(self, text):
        """
        é€šç”¨å¸ƒå°”æ„å›¾åˆ¤æ–­å™¨ã€‚
        åˆ¤æ–­ç”¨æˆ·æ˜¯å¦è¡¨è¾¾äº†ã€è‚¯å®š/åŒæ„/ç¡®è®¤ã€‘çš„æ€åº¦ã€‚
        """
        if not text: return False
        t = text.strip().lower()
        
        # 1. å¦å®šå¿«ç­› (Fast Negative) - åªè¦æ²¾è¾¹å°±æ‹’ï¼Œçœæµ
        neg_kw = ["ä¸", "å¦", "åˆ«", "å–æ¶ˆ", "no", "n", "cancel", "ç®—äº†", "æ”¾å¼ƒ"]
        # å¦‚æœæ˜¯çŸ­è¯­ä¸”åŒ…å«å¦å®šè¯ï¼Œç›´æ¥ False
        if len(t) < 10 and any(k in t for k in neg_kw):
            return False

        # 2. è‚¯å®šå¿«ç­› (Fast Positive)
        aff_kw = ["æ˜¯", "æ”¹", "è¦†ç›–", "å¯¹", "yes", "y", "ok", "ç¡®è®¤", "å¥½", "å—¯", "å¦¥", "å­˜", "å…³è”", "æ–°å»º"]
        if len(t) < 10 and any(k in t for k in aff_kw):
            return True

        # 3. å¤æ‚æƒ…å†µäº¤ç»™ LLM (AI ä»²è£)
        from src.services.llm_service import judge_affirmative
        return judge_affirmative(text, self.api_key, self.endpoint_id)

    def detect_data_conflicts(self, old_data, new_data):
        """
        å¯¹æ¯”æ–°æ—§æ•°æ®ï¼Œæ£€æµ‹å­—æ®µå†²çªã€‚
        è¿”å›: list of (category, key, field_name, old_val, new_val)
        """
        conflicts = []
        
        # å­—æ®µåç§°æ˜ å°„ (ç”¨äºæ˜¾ç¤º)
        field_labels = {
            "budget": "é¢„ç®—é‡‘é¢",
            "opportunity_stage": "å•†æœºé˜¶æ®µ",
            "timeline": "æ—¶é—´èŠ‚ç‚¹",
            "procurement_process": "é‡‡è´­æµç¨‹",
            "payment_terms": "ä»˜æ¬¾æ–¹å¼",
            "name": "å®¢æˆ·å§“å",
            "company": "å®¢æˆ·å…¬å¸",
            "role": "å®¢æˆ·èŒä½",
            "contact": "è”ç³»æ–¹å¼"
        }

        # 1. æ£€æŸ¥ project_opportunity
        old_opp = old_data.get("project_opportunity", {})
        new_opp = new_data.get("project_opportunity", {})
        
        for k, v_new in new_opp.items():
            # å¿½ç•¥éä¸šåŠ¡å­—æ®µ
            if k in ["is_new_project", "project_name"]: continue
            # å¦‚æœæ–°å€¼ä¸ºç©ºï¼Œä¸è§†ä¸ºå†²çªï¼ˆä¸è¦†ç›–ï¼‰
            if not v_new or v_new == "null": continue
            
            v_old = old_opp.get(k)
            # è½¬å­—ç¬¦ä¸²æ¯”è¾ƒï¼Œå¿½ç•¥ç±»å‹å·®å¼‚
            if str(v_new) != str(v_old) and v_old:
                label = field_labels.get(k, k)
                conflicts.append(("project_opportunity", k, label, v_old, v_new))

        # 2. æ£€æŸ¥ customer_info
        old_cust = old_data.get("customer_info", {})
        new_cust = new_data.get("customer_info", {})
        
        for k, v_new in new_cust.items():
            if not v_new: continue
            v_old = old_cust.get(k)
            if str(v_new) != str(v_old) and v_old:
                label = field_labels.get(k, k)
                conflicts.append(("customer_info", k, label, v_old, v_new))

        return conflicts

    def resolve_target_interactive(self, content, current_context_id=None):
        """
        [æ ¸å¿ƒä¸šåŠ¡é€»è¾‘] ç›®æ ‡è§£ææµç¨‹
        ç»Ÿä¸€å°è£… CLI/GUI çš„æŸ¥æ‰¾é€»è¾‘ï¼šæå– -> ä¸Šä¸‹æ–‡æ£€æŸ¥ -> æœç´¢ -> ç»“æœåˆ¤å®š
        
        Returns:
            (target_obj, candidates_list, status_code)
            status_code: "found_by_context", "found_exact", "ambiguous", "not_found"
        """
        search_term = self.extract_search_term(content)
        
        # 1. ä¸Šä¸‹æ–‡æ¨¡ç³Šæ£€æŸ¥ (Vague Check)
        is_vague = not search_term or any(k in search_term.lower() for k in ["unknown", "è®°å½•", "é¡¹ç›®", "ä¿®æ”¹", "æ›´æ–°", "å†…å®¹"])
        
        if is_vague and current_context_id:
            target = self.get_opportunity_by_id(current_context_id)
            if target:
                return target, [], "found_by_context"
        
        # 2. ä¸¥æ ¼æœç´¢
        final_term = search_term if search_term else content
        candidates = self.find_potential_matches(final_term)
        
        if not candidates:
            return None, [], "not_found"
            
        if len(candidates) == 1:
            target = self.get_opportunity_by_id(candidates[0]["id"])
            if target:
                return target, [], "found_exact"
        
        # 3. å¤šç»“æœæ­§ä¹‰
        return None, candidates, "ambiguous"

    def process_list_request(self, content):
        """
        [æ ¸å¿ƒä¸šåŠ¡é€»è¾‘] å¤„ç†å•†æœºåˆ—è¡¨æŸ¥è¯¢
        """
        search_term = self.extract_search_term(content) or ""
        clean_term = search_term.upper().replace("`", "").replace("'", "").replace('"', "")
        
        is_full_list = not clean_term or clean_term in ["ALL", "æœªçŸ¥", "UNKNOWN", "å•†æœº", "é¡¹ç›®", "åˆ—è¡¨", "å…¨éƒ¨", "æ‰€æœ‰"]
        
        if is_full_list:
            results = self.list_opportunities()
        else:
            def simple_filter(data): 
                return search_term.lower() in json.dumps(data, ensure_ascii=False).lower()
            results = self.list_opportunities(simple_filter)
            
        return {
            "results": results,
            "message": f"ğŸ“‹ æ‰¾åˆ° {len(results)} æ¡å•†æœº" if results else "æš‚æœªæ‰¾åˆ°ç›¸å…³å•†æœºã€‚",
            "search_term": search_term if not is_full_list else "å…¨éƒ¨"
        }

    def get_missing_fields_notification(self, data):
        """
        [ç»Ÿä¸€è¯æœ¯é€»è¾‘] ç”Ÿæˆç¼ºå¤±å­—æ®µçš„é€šçŸ¥æ–‡æœ¬
        """
        missing = self.get_missing_fields(data)
        if not missing:
            return "âœ… ä¿¡æ¯å®Œæ•´ã€‚ç¡®è®¤æ— è¯¯è¯·æ‰§è¡Œä¿å­˜ã€‚"
            
        names = [v[0] for v in missing.values()]
        return f"âš ï¸ å½“å‰è‰ç¨¿ç¼ºå¤±å…³é”®ä¿¡æ¯ï¼š**{', '.join(names)}**ã€‚\næ‚¨å¯ä»¥ç›´æ¥åœ¨å¯¹è¯æ¡†è¾“å…¥è¡¥å……ï¼ˆå¦‚â€œé¢„ç®—50ä¸‡â€ï¼‰ï¼Œæˆ–ç›´æ¥æ‰§è¡Œä¿å­˜ã€‚"

    # --- V3.0 ç¬”è®°æš‚å­˜ä¸æäº¤é€»è¾‘ ---

    def add_to_note_buffer(self, content):
        """å°†ä¸€æ®µå½•å…¥å†…å®¹æ·»åŠ åˆ°ç¬”è®°æš‚å­˜åŒº"""
        polished = self.polish(content)
        self.note_buffer.append(polished)
        return polished

    def clear_note_buffer(self):
        """æ¸…ç©ºç¬”è®°æš‚å­˜åŒº"""
        self.note_buffer = []

    def process_commit_request(self, project_name_hint=None):
        """
        [æ ¸å¿ƒä¸šåŠ¡é€»è¾‘] å°†æš‚å­˜åŒºçš„ç¬”è®°æ­£å¼æäº¤åˆ°å•†æœºã€‚
        1. å°è¯•é”å®šç›®æ ‡å•†æœº (æ ¹æ® hint æˆ–ç¬”è®°å†…å®¹)
        2. è°ƒç”¨ Architect Analyze è¿›è¡Œç»“æ„åŒ–æå–/åˆå¹¶
        3. è¿”å›ç»“æœåŒ…
        """
        if not self.note_buffer:
            return {"status": "error", "message": "ç¬”è®°æš‚å­˜åŒºä¸ºç©ºï¼Œè¯·å…ˆå½•å…¥ä¸€äº›å†…å®¹ã€‚"}

        # 1. é”å®šç›®æ ‡
        target_obj = None
        if project_name_hint:
            # è¿™é‡Œçš„ hint å¯èƒ½æ˜¯ä»æ„å›¾è¯†åˆ«é‡Œæ‹¿å‡ºæ¥çš„ "RECORD" content
            res_target, candidates, status = self.resolve_target_interactive(project_name_hint)
            if status in ["found_exact", "found_by_context"]:
                target_obj = res_target

        # 2. å¦‚æœæ²¡é”å®šï¼Œå…ˆåšä¸€æ¬¡åˆæ­¥åˆ†æçœ‹çœ‹ç¬”è®°é‡Œæåˆ°äº†å“ªä¸ªé¡¹ç›®
        if not target_obj:
            # ä¸´æ—¶ç”Ÿæˆä¸€ä¸ªè‰ç¨¿æ¥æ¢æ¢è·¯ (å–å‰ 3 æ¡ç¬”è®°)
            preview = architect_analyze(self.note_buffer[:3], self.api_key, self.endpoint_id, recorder=self.default_recorder)
            if preview:
                extracted_name = preview.get("project_opportunity", {}).get("project_name")
                if extracted_name:
                    res_target, candidates, status = self.resolve_target_interactive(extracted_name)
                    if status in ["found_exact", "found_by_context"]:
                        target_obj = res_target

        # 3. è°ƒç”¨é”€å”®æ¶æ„å¸ˆè¿›è¡Œæœ€ç»ˆå¤„ç†
        # ä¼ å…¥ target_obj (å¦‚æœæœ‰) è¿›è¡Œåˆå¹¶ï¼Œå¦åˆ™ä¸ºæ–°å»º
        result_json = architect_analyze(
            self.note_buffer, 
            self.api_key, 
            self.endpoint_id, 
            original_data=target_obj, 
            recorder=self.default_recorder
        )

        if not result_json:
            return {"status": "error", "message": "AI æäº¤å¤„ç†å¤±è´¥ã€‚"}

        # 4. æŸ¥é‡åˆ¤å®š (å¦‚æœæ˜¯æ–°å»ºæ¨¡å¼ï¼Œå¯èƒ½è¿˜éœ€è¦æ£€æŸ¥æ˜¯å¦æœ‰åŒåé¡¹ç›®)
        status = "new"
        linked_target = None
        if target_obj:
            status = "linked"
            linked_target = {"id": target_obj["id"], "name": target_obj.get("project_name", "æœªçŸ¥")}
        else:
            # å³ä½¿ architect æ²¡æ‹¿åˆ° original_jsonï¼Œå®ƒå¯èƒ½è¾“å‡ºäº†ä¸€ä¸ªå·²å­˜åœ¨çš„é¡¹ç›®å
            # è¿™é‡Œå†åšæœ€åä¸€å±‚ä¿é™©
            p_name = result_json.get("project_opportunity", {}).get("project_name")
            if p_name:
                matches = self.find_potential_matches(p_name)
                for m in matches:
                    if m["name"].strip().lower() == p_name.strip().lower():
                        linked_target = m
                        status = "linked"
                        result_json["id"] = m["id"]
                        break

        return {
            "status": status,
            "draft": result_json,
            "linked_target": linked_target,
            "missing_fields": self.get_missing_fields(result_json)
        }